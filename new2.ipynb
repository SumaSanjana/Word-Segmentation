{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mvy48\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "dataset_path = \"C:\\\\Users\\\\mvy48\\\\Downloads\\\\ml_combined_anoop-cc-gokul_07Dec19.txt\"\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "lines = lines[:200000]\n",
    "\n",
    "def label_sentences(sentences):\n",
    "    labeled_data = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        words = sentence.split()\n",
    "        labels = []\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                labels.append((word, 'S'))\n",
    "            else:\n",
    "                labels.append((word[0], 'B'))\n",
    "                for char in word[1:-1]:\n",
    "                    labels.append((char, 'I'))\n",
    "                labels.append((word[-1], 'E'))\n",
    "        labeled_data.append(labels)\n",
    "    return labeled_data\n",
    "\n",
    "labeled_lines = label_sentences(lines)\n",
    "\n",
    "sentences = [[char for char, label in sentence] for sentence in labeled_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = labeled_lines[0]  # Taking the first sentence as an example\n",
    "sample_sentence_processed = [(char + '/' + tag) for char, tag in sample_sentence]\n",
    "print(\" \".join(sample_sentence_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "vocab = list(word2vec_model.wv.index_to_key)\n",
    "embeddings = [word2vec_model.wv[word] for word in vocab]\n",
    "embedding_df = pd.DataFrame(embeddings, index=vocab)\n",
    "embedding_df.to_csv('word_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Characters to Indices and Prepare Labels\n",
    "char_to_index = {char: idx for idx, char in enumerate(word2vec_model.wv.index_to_key, start=1)}\n",
    "char_to_index['PAD'] = 0\n",
    "\n",
    "label_to_index = {'B': 0, 'I': 1, 'E': 2, 'S': 3}\n",
    "\n",
    "def prepare_data(labeled_sentences, char_to_index, label_to_index):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in labeled_sentences:\n",
    "        sentence_indices = [char_to_index[char] for char, label in sentence]\n",
    "        label_indices = [label_to_index[label] for char, label in sentence]\n",
    "        X.append(sentence_indices)\n",
    "        y.append(label_indices)\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_data(labeled_lines, char_to_index, label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding='post', value=char_to_index['PAD'])\n",
    "y_padded = pad_sequences(y, maxlen=MAX_LEN, padding='post', value=label_to_index['S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_padded, test_size=0.3, random_state=42)\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "y_test = np.expand_dims(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mvy48\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mvy48\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          10100     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 128)          84480     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 100, 4)            516       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95096 (371.47 KB)\n",
      "Trainable params: 95096 (371.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(char_to_index)\n",
    "output_size = len(label_to_index)\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_LEN, mask_zero=True),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1)),\n",
    "    TimeDistributed(Dense(output_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\mvy48\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mvy48\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1261/1969 [==================>...........] - ETA: 2:48 - loss: 0.2239 - accuracy: 0.9193"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_true = np.squeeze(y_test, axis=-1)\n",
    "\n",
    "# Classification Report\n",
    "label_names = ['B', 'I', 'E', 'S']\n",
    "print(classification_report(y_true.flatten(), y_pred.flatten(), target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds, index_to_label):\n",
    "    label_preds = np.argmax(preds, axis=-1)\n",
    "    return [index_to_label[idx] for idx in label_preds[0]]\n",
    "\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "\n",
    "def reconstruct_sentence(chars, labels):\n",
    "    words = []\n",
    "    word = ''\n",
    "    for char, label in zip(chars, labels):\n",
    "        if label == 'B':\n",
    "            if word:\n",
    "                words.append(word)\n",
    "            word = char\n",
    "        elif label == 'I':\n",
    "            word += char\n",
    "        elif label == 'E':\n",
    "            word += char\n",
    "            words.append(word)\n",
    "            word = ''\n",
    "        elif label == 'S':\n",
    "            if word:\n",
    "                words.append(word)\n",
    "            words.append(char)\n",
    "            word = ''\n",
    "    if word:\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "def predict_sentence(sentence, model, char_to_index, index_to_label):\n",
    "    sentence_indices = [char_to_index.get(char, 0) for char in sentence]\n",
    "    sentence_padded = pad_sequences([sentence_indices], maxlen=MAX_LEN, padding='post', value=char_to_index['PAD'])\n",
    "    preds = model.predict(sentence_padded)\n",
    "    label_preds = decode_predictions(preds, index_to_label)\n",
    "    segmented_words = reconstruct_sentence(sentence, label_preds)\n",
    "    segmented_labels = label_preds\n",
    "    return segmented_words, segmented_labels\n",
    "\n",
    "# Define the sentence to predict\n",
    "sentence_to_predict = \"ആഗ്രഹങ്ങൾസാക്ഷാത്കരിക്കാന്പഠിക്കണം\"\n",
    "\n",
    "# Make Prediction\n",
    "predicted_words, predicted_labels = predict_sentence(sentence_to_predict, model, char_to_index, index_to_label)\n",
    "\n",
    "# Print the Segmented Sentence and Labels\n",
    "print(\"Segmented Sentence:\", ' '.join(predicted_words))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
